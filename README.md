SpicyChat: Health-Focused Recipe LLM Chatbot AI Assistant
SpicyChat is an educational, spare-time project showcasing the end-to-end pipeline of creating a specialised Large Language Model (LLM) for health and nutrition-conscious recipe generation.

It was successfully trained on UCL compute resources (an RTX 3090 Ti GPU) to demonstrate the efficiency and effectiveness of LoRA fine-tuning on large foundation models.

Project Details
Metric

Value

Base Model

Meta LLaMA 3.1 8B Instruct

Training Method

LoRA (Low-Rank Adaptation)

Training Device

NVIDIA GeForce RTX 3090 Ti (24GB VRAM)

Training Time

Approx. 26 minutes (for 3 epochs)

Final Loss

~0.7075 (Indicating successful specialisation)

Core Goal

Adapt LLaMA to follow the strict structure: Ingredients â†’ Structured Recipe.

1. Full Replication Guide
To replicate this project and run the model, you must complete the following setup steps.

Data and Model Preparation
Ensure your Python environment is active (source spicyVenv/bin/activate) and your Hugging Face token is exported.

Script

Source & Requirement

Command

Download & Parse Dataset

Gets raw data from the VincentLimbach/Cooking Hugging Face repo.

python scripts/download_and_parse_cooking.py

Download LLaMA Model

Gets model weights from the Meta/LLaMA 3.1 Hugging Face repo. Requires HF API Key/Access.

python scripts/download_llama31.py

Training
This command executes the fine-tuning process. The nohup command ensures the job continues running reliably in the background, even if the user disconnects the SSH session (No Hang Up).

# Run training in the background, saving output to training.log
nohup python scripts/train_cooking_model.py > training.log 2>&1 &

2. Testing Dev Product (No Live Web Demo)
Due to the resource/cost requirements of running a live 16GB Large Language Model, providing a public web demo via the Flask API is not feasible. You can successfully mimic the full live experience by running the provided Command Line Interface (CLI) tool on any machine with sufficient GPU capacity.

The final product is therefore demonstrated via the interactive Command Line Interface (CLI) tester.

Usage: Interactive CLI Tester
![Example1: API Verification](readme_images/screenshot1.png)

This is the primary way to test the model's specialized output.

# Execute the CLI Tester (Must be run from the project root after VENV activation)
python scripts/test_spicychat_cli.py

Example Interaction (CLI Test)
![Example2 : cURL Verification](readme_images/screenshot1.png)

This demonstrates the core functional test of the final product: loading the LoRA adapter and generating a response.

API Verification (Internal)
This shows the raw cURL command used during internal testing to verify the Flask API server was functional.

# Send a test request to the running API server (if temporarily active)
curl -X POST [http://127.0.0.1:5000/generate](http://127.0.0.1:5000/generate) \
     -H "Content-Type: application/json" \
     -d '{"ingredients": "chicken breast, rice, onions, teriyaki sauce, chilli peppers"}'

Example API Output (cURL Test)

This is the JSON recipe generated by the model during the internal API test.
